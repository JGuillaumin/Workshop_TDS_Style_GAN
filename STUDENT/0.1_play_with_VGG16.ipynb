{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.misc import imread, imresize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our tool box\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some images :\n",
    "dog_path = '../images/dog.jpg'\n",
    "golden_path = '../images/golden_retriever.jpg'\n",
    "laska_path = '../images/laska.png'\n",
    "train_path = '../images/hongkong.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documenation \n",
    "- doc VGG16 : \n",
    "- doc preprocess_input :\n",
    "- doc decode_predictions :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use VGG16 to classify images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = VGG16(?????)\n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imread(golden_path)\n",
    "plt.imshow(img)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to resize the image ???\n",
    "img = ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply preprocessing (specific to the training procedure used with VGG16/19)\n",
    "# RGB -> BGR + mean subtraction \n",
    "img_pr = preprocess_input(img.astype(np.float32))\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "_ = plt.hist(img.flatten(), bins=255, normed=True)\n",
    "plt.subplot(122)\n",
    "_ = plt.hist(img_pr.flatten(), bins=255, normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = vgg16.predict(????)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which the predicted class ? (np.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or use the built-in function to decode the predictions : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Visualization \n",
    "\n",
    "- pre-trained VGG16 without the dense layers\n",
    "- with a different shape \n",
    "\n",
    "- 'Model' objects get 'layers' attribute and 'get_layer(name)' method\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imread(golden_path)\n",
    "plt.imshow(img)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new VGG16 \n",
    "vgg16 = VGG16(?????)\n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the layers are named\n",
    "[(layer.name, layer) for layer in vgg16.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block2_conv2 = vgg16.get_layer(????)\n",
    "print(block2_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new 'Model':\n",
    "# - take as input the same input of vgg16\n",
    "# - as output : a specific layer\n",
    "\n",
    "features_extractor = Model(inputs=????, outputs=????)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pr = preprocess_input(img.astype(np.float32))\n",
    "features = features_extractor.predict(????)[0]\n",
    "print(\"features : {} |  {}\".format(features.shape, features.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the first feature as gray-level image \n",
    "plt.imshow(features[:, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will plot all the feature maps within a fig of size (20,20)\n",
    "def plot_feature_maps(feature_maps):\n",
    "    height, width, depth = feature_maps.shape\n",
    "    nb_plot = int(np.rint(np.sqrt(depth)) + 1)\n",
    "    fig = plt.figure(figsize=(25, 25))\n",
    "    for i in range(depth):\n",
    "        plt.subplot(nb_plot, nb_plot, i+1)\n",
    "        plt.imshow(feature_maps[:,:,i], cmap='gray')\n",
    "        plt.title('feature map {}'.format(i+1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_maps(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
